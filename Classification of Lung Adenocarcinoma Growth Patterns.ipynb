{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellOMaps: A Compact Representation for Robust Classification of Lung Adenocarcinoma Growth Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create CellOMaps and use them for growth pattern classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellOMaps Construction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Perform nuclie segmentation and classification using Hover-Net. Implementation can be found in the TIA toolbox here: (https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/08-nucleus-instance-segmentation.html)\n",
    "To insure compatability with this notbook we recommend using pretrained_model=\"hovernet_fast-pannuke\", and inference on 0.5 mpp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreatingCentroidsMasksFromHoverNet import create_maks\n",
    "path_to_hoverNet_output = 'Cell_predictions/'  \n",
    "path_to_cell_masks= 'cellmaps/' \n",
    "path_to_slides = 'Data/slides/'\n",
    "create_maks(path_to_slides, path_to_hoverNet_output, path_to_cell_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth Pattern classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary packages and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import auc, roc_curve, f1_score, precision_recall_curve, average_precision_score, roc_auc_score, \\\n",
    "    confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tiatoolbox.wsicore import WSIReader\n",
    "from tiatoolbox.tools import patchextraction\n",
    "from torch.utils.data import ConcatDataset\n",
    "from tiatoolbox.annotation.storage import Annotation, SQLiteStore\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import cv2\n",
    "import csv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "device = torch.device(\"cuda:0\")\n",
    "multi_gpu = False\n",
    "\n",
    "\n",
    "dataPath = 'Data/'\n",
    "output = 'output'\n",
    "path_to_centroids_masks = 'cellmaps/'\n",
    "\n",
    "\n",
    "all_centroid_masks = os.listdir(path_to_centroids_masks)\n",
    "all_centroid_masks = [os.path.join(path_to_centroids_masks, m) for m in all_centroid_masks]\n",
    "\n",
    "num_trials = 7\n",
    "runs = 1\n",
    "batch_size = 5\n",
    "nepochs = 50\n",
    "learning_rate = 1e-5\n",
    "workers = 12\n",
    "test_every = 1\n",
    "patch_size = (448, 448, 3)  # this is calculated at approximately 5x\n",
    "patch_size1d,_,_ = patch_size  # this is calculated at approximately 5x\n",
    "mask_percentage = 0.4\n",
    "# Dialation parameters\n",
    "kernel_size = 1\n",
    "dialation_iterations = 2\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * kernel_size + 1, 2 * kernel_size + 1))\n",
    "focal_loss_gamma = 0.7\n",
    "cellMapChannels = 4\n",
    "\n",
    "labelDict = {'Solid': 0, 'Acinar': 1, 'Papillary': 2, 'Micropapillary': 3, 'Lepidic': 4, 'Normal': 5}\n",
    "label_dict = {\n",
    "    0: \"Solid\",\n",
    "    1: \"Acinar\",\n",
    "    2: \"Papillary\",\n",
    "    3: \"Micropapillary\",\n",
    "    4: \"Lepidic\",\n",
    "    5: \"Normal\"\n",
    "}\n",
    "numClasses = len(labelDict)\n",
    "\n",
    "# logging results ...\n",
    "csv_file_path = os.path.join(output, 'Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TileDataset(data.Dataset):\n",
    "    def __init__(self, tiles, labels, transform, version):\n",
    "        self.tiles = tiles\n",
    "        self.targets = labels\n",
    "        self.transform = transform\n",
    "        self.version = version\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tile = self.tiles[index]\n",
    "        patch = tile['patch']\n",
    "        target = self.targets[index]\n",
    "        img = cv2.dilate(patch,kernel, iterations=dialation_iterations)\n",
    "        img = Image.fromarray(img)\n",
    "        if img.width < patch_size1d or img.height < patch_size1d:\n",
    "            slide = tile['slideName']\n",
    "            print(f'Small patch found in slide {slide} pattern {label_dict[target]} with size {img.width}x{img.height}')\n",
    "            img = img.resize((patch_size1d,patch_size1d), Image.BICUBIC)\n",
    "\n",
    "        img = self.transform(img)  # Used to be before the dialation\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = torch.randperm(len(self.targets))\n",
    "        self.tiles = [self.tiles[i] for i in indices]\n",
    "        self.targets = [self.targets[i] for i in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patchExtraction(img, mask, slide_name):\n",
    "    # Extract patches\n",
    "    slides_path = os.path.join(dataPath, 'slides')\n",
    "    slide_path = f'{slides_path}/{slide_name}.svs'\n",
    "    wsi = WSIReader.open(slide_path)\n",
    "    slide_dimentions = wsi.slide_dimensions(2 , 'mpp')\n",
    "    # resize the cell map (img) to insure dimentions compatability\n",
    "    img_pil = Image.fromarray(img)\n",
    "    resized_image = img_pil.resize(slide_dimentions, Image.BICUBIC)\n",
    "    resized_cellMap = np.array(resized_image)\n",
    "\n",
    "    patchextractor = patchextraction.SlidingWindowPatchExtractor(\n",
    "        input_img=slide_path,\n",
    "        patch_size=(patch_size1d, patch_size1d),\n",
    "        input_mask=mask, resolution=2, units='mpp',\n",
    "        stride=(patch_size1d, patch_size1d), min_mask_ratio=mask_percentage)\n",
    "\n",
    "    coordinates = patchextractor.coordinate_list\n",
    "\n",
    "    # Create an empty list to store the patches\n",
    "    patches = []\n",
    "    # Extract patches\n",
    "    for patch_coor in coordinates:\n",
    "        patch = resized_cellMap[patch_coor[1]:patch_coor[3], patch_coor[0]:patch_coor[2], :]\n",
    "        tile = {'x_start': patch_coor[0], 'x_end': patch_coor[2], 'y_start': patch_coor[1], 'y_end': patch_coor[3], 'patch': patch,\n",
    "                'slideName': slide_name}\n",
    "        patches.append(tile)\n",
    "\n",
    "    return patches\n",
    "\n",
    "def prep_data(masks):\n",
    "    data = []\n",
    "    labels = []\n",
    "    counts_per_class = {'Solid': 0, 'Acinar': 0, 'Papillary': 0, 'Micropapillary': 0, 'Lepidic': 0, 'Normal': 0,\n",
    "                        'other': 0}\n",
    "\n",
    "    for maskP in masks:\n",
    "        mask = os.path.basename(maskP)\n",
    "        # Extract label\n",
    "        mask = os.path.splitext(mask)[0]\n",
    "        label = mask.split('_')[-1]\n",
    "        mask_name = mask.split('_')[0]\n",
    "\n",
    "        # find the corresponding cellMaps, stack them, then tile it\n",
    "        tdata = []\n",
    "        tlab = []\n",
    "        img_layers = []\n",
    "        for centroid_mask in all_centroid_masks:\n",
    "            centroid_mask_slide = os.path.basename(centroid_mask)\n",
    "            if centroid_mask_slide.startswith(mask_name):\n",
    "                if 'Neoplastic'  in centroid_mask_slide  or 'ConnectiveSoftTissue_Centroids' in centroid_mask_slide :\n",
    "                    img_layers.append(centroid_mask)\n",
    "\n",
    "        if len(img_layers) == 0:\n",
    "            print('Slide#: {} has No hoverNet Features ! \\n'.format(mask_name))\n",
    "            continue\n",
    "        img_layers.sort()  # To insure all instance are in the same order\n",
    "        images = [Image.open(maskp) for maskp in img_layers]\n",
    "        images = [im.convert('L') for im in images]\n",
    "        width, height = images[0].size\n",
    "        stacked_array = np.empty((height, width, 3), dtype=np.uint8)\n",
    "        for i, image in enumerate(images):\n",
    "            stacked_array[:, :, i] = np.array(image)\n",
    "\n",
    "        mask_gp = Image.open(maskP).convert('L')\n",
    "        mask_gp = mask_gp.resize(images[0].size)\n",
    "        image_gp_array = np.array(mask_gp)\n",
    "        new_mask = image_gp_array\n",
    "        new_mask[new_mask > 0] = 1\n",
    "\n",
    "        patches = patchExtraction(stacked_array, new_mask, mask_name)\n",
    "        for patch in patches:\n",
    "            tdata.append(patch)\n",
    "            tlab.append(labelDict[label])\n",
    "\n",
    "        # Display the number of patches\n",
    "        print(f\"slide {mask_name}: Number of patches: {len(patches)} for class: {label}\")\n",
    "        counts_per_class[label] = counts_per_class[label] + len(patches)\n",
    "        data.extend(tdata)\n",
    "        labels.extend(tlab)  # tile ground truth label\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def append_to_csv(trial, epoch, unseen_slides, aucroc, f1, accuracy, precision, recall, pr_macro, pr_micro, acc_per_class, f1_per_class):\n",
    "    solid, acinar, papillary, micropapillary, lepidic, normal = acc_per_class\n",
    "    f1_solid, f1_acinar, f1_papillary, f1_micropapillary, f1_lepidic, f1_normal = f1_per_class\n",
    "    data = [trial, epoch,len(unseen_slides), unseen_slides, aucroc, f1, accuracy, precision, recall, pr_macro, pr_micro, solid, acinar, papillary, micropapillary, lepidic, normal, f1_solid, f1_acinar, f1_papillary, f1_micropapillary, f1_lepidic, f1_normal]\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(csv_file_path, 'r') as file:\n",
    "            file_exists = True\n",
    "    except FileNotFoundError:\n",
    "        file_exists = False\n",
    "\n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header row if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Trial', 'epoch', 'count', 'unseen_slides', 'AUC-ROC', 'F1_macro', 'Accuracy', 'Precision', 'Recall', 'AUCPR_macro', 'AUCPR_micro', 'solid','acinar', 'papillary', 'micropapillary','lepidic','normal', 'f1_solid', 'f1_acinar', 'f1_papillary', 'f1_micropapillary', 'f1_lepidic', 'f1_normal'])\n",
    "        writer.writerow(data)\n",
    "\n",
    "\n",
    "def stratified_split(exclusions=[]):\n",
    "    slides_path = os.path.join(dataPath, 'slides')\n",
    "    masks_path = os.path.join(dataPath, 'masks')\n",
    "    masks = os.listdir(masks_path)\n",
    "    img_paths = os.listdir(slides_path)\n",
    "    labels = []\n",
    "    required_slides=[]\n",
    "    img_paths = [os.path.splitext(name)[0] for name in img_paths]\n",
    "\n",
    "    for slide in img_paths:\n",
    "        if slide in exclusions:\n",
    "            continue\n",
    "        slide_masks = [m for m in masks if m.startswith(slide)]\n",
    "        slide_masks = [string.split('_')[1].split('.')[0] for string in slide_masks]\n",
    "\n",
    "        slide_labels = [labelDict[mask] for mask in slide_masks]\n",
    "        labels.append(slide_labels)\n",
    "        required_slides.append(slide)\n",
    "\n",
    "    label_binarizer = MultiLabelBinarizer()\n",
    "    label_binarizer.fit(labels)\n",
    "    label_matrix = label_binarizer.transform(labels)\n",
    "    label_matrix = np.array(label_matrix)\n",
    "\n",
    "    required_slides = np.array(required_slides)\n",
    "    img_paths_2d = required_slides.reshape(-1, 1)\n",
    "\n",
    "    # Shuffle the data to obtain diffrent splits each time\n",
    "    shuffled_indices = np.random.permutation(len(img_paths_2d))\n",
    "    shuffled_data = img_paths_2d[shuffled_indices]\n",
    "    shuffled_labels = label_matrix[shuffled_indices]\n",
    "    test_size = 0.19\n",
    "\n",
    "    train_paths, _, test_paths, _ = iterative_train_test_split(shuffled_data, shuffled_labels, test_size=test_size)\n",
    "\n",
    "    return test_paths\n",
    "\n",
    "def train_valid_split(x, y, seed=5):\n",
    "    outer_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=0.1, random_state=seed\n",
    "    )\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    for train_index, test_index in outer_splitter.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def inference(run, loader, model, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    running_acc = 0.\n",
    "    running_loss = 0.\n",
    "    probs = torch.FloatTensor(len(loader.dataset), numClasses)\n",
    "    preds = torch.FloatTensor(len(loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target) in enumerate(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = torch.tensor(target).to(device)\n",
    "            output = model(inputs)\n",
    "            y = F.softmax(output, dim=1)\n",
    "\n",
    "            loss = criterion(y, target)\n",
    "            acc = calculate_accuracy(output, target)\n",
    "\n",
    "            _, pr = torch.max(output, 1)\n",
    "\n",
    "            preds[i * batch_size:i * batch_size + inputs.size(0)] = pr.detach().clone()\n",
    "            probs[i * batch_size:i * batch_size + inputs.size(0)] = y.detach().clone()\n",
    "            running_acc += (acc.item()) * inputs.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            if i % 100 == 0:\n",
    "                print('Inference\\tEpoch: [{:3d}/{:3d}]\\tBatch: [{:3d}/{}]\\t Validatoin: Loss: {:.4f}, acc: {:0.2f}%'\n",
    "                      .format(run + 1, nepochs, i + 1, len(loader), running_loss / ((i + 1) * inputs.size(0)),\n",
    "                              (100 * running_acc) / ((i + 1) * inputs.size(0))))\n",
    "\n",
    "    return probs.cpu().numpy(), running_loss / len(loader.dataset), running_acc / len(\n",
    "        loader.dataset), preds.cpu().numpy()\n",
    "\n",
    "\n",
    "# cnn training\n",
    "def train(run, loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    for i, (inputs, target) in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = torch.tensor(target).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        y = F.softmax(output, dim=1)\n",
    "\n",
    "        if torch.isnan(y).any():\n",
    "            print(inputs)\n",
    "            print(y)\n",
    "\n",
    "        loss = criterion(y, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        acc = calculate_accuracy(output, target)\n",
    "        running_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Train Epoch : [{:3d}/{:3d}] Batch number: {:3d}, Training: Loss: {:.4f}, acc: {:.2f}%\".\n",
    "                  format(run + 1, nepochs, i + 1, running_loss / ((i + 1) * inputs.size(0)),\n",
    "                         (100 * running_acc) / ((i + 1) * inputs.size(0))))\n",
    "\n",
    "    return running_loss / len(loader.dataset), running_acc / len(loader.dataset)\n",
    "\n",
    "\n",
    "def calc_AUCPR(target, probs):\n",
    "    # Convert y_true to one-hot encoded format\n",
    "    y_true = label_binarize(target, classes=np.arange(numClasses))\n",
    "\n",
    "    # Initialize variables to store precision, recall, and average precision for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "\n",
    "    # Compute precision, recall, and average precision for each class\n",
    "    for i in range(numClasses):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true[:, i], probs[:, i])\n",
    "        average_precision[i] = average_precision_score(y_true[:, i], probs[:, i])\n",
    "\n",
    "    # Compute micro-average precision and recall\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(), probs.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(y_true, probs, average=\"micro\")\n",
    "\n",
    "    # Compute macro-average precision and recall\n",
    "    # Here, you can also calculate the mean of average_precision[i] values for each class\n",
    "    average_precision[\"macro\"] = np.mean(list(average_precision.values()))\n",
    "\n",
    "    return round(average_precision[\"macro\"], 2), round(average_precision[\"micro\"], 2)\n",
    "\n",
    "\n",
    "\n",
    "def calc_metrics(target, prediction, probs):\n",
    "    # calculate the ROC AUC score\n",
    "    roc_auc = roc_auc_score(target, probs, multi_class='ovo')\n",
    "\n",
    "    f1 = f1_score(target, prediction, average='macro')\n",
    "    precision = precision_score(target, prediction, average='macro')\n",
    "    recall = recall_score(target, prediction, average='macro')\n",
    "\n",
    "    return f1, roc_auc, precision, recall\n",
    "\n",
    "\n",
    "def calculate_accuracy(output, target):\n",
    "    preds = output.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(target.view_as(preds)).sum()\n",
    "    acc = correct.float() / preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "def calculate_accuracy_f1_perClass(output, target):\n",
    "    output = np.array(output)\n",
    "    target = np.array(target)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for class_index in range(numClasses):\n",
    "        class_mask = (target == class_index)  # Create a mask for samples of class i\n",
    "        class_true = target[class_mask]\n",
    "        class_pred = output[class_mask]\n",
    "        class_accuracy = accuracy_score(class_true, class_pred)  # Calculate accuracy for class i\n",
    "        class_f1 = f1_score(class_true, class_pred, average='weighted')  # Calculate F1 score for class i\n",
    "        accuracies.append(round(class_accuracy, 2))\n",
    "        f1_scores.append(round(class_f1, 2))\n",
    "\n",
    "    return accuracies, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempout = output\n",
    "\n",
    "for trial in range(num_trials):\n",
    "\n",
    "    # Stratify the data\n",
    "    unseen_test_set_1 = stratified_split()\n",
    "    unseen_test_set = unseen_test_set_1.flatten()\n",
    "\n",
    "    # Training data\n",
    "    masksp = os.path.join(dataPath, 'masks')\n",
    "    masks = os.listdir(masksp)\n",
    "\n",
    "\n",
    "    training_masks = np.array([x for x in masks if not any(x.startswith(prefix) for prefix in unseen_test_set)])\n",
    "    training_masks = [os.path.join(masksp, m) for m in training_masks]\n",
    "\n",
    "    train_valid_data, train_valid_labels = prep_data(training_masks)\n",
    "    # splitting into train and validation\n",
    "    train_data, train_labels, valid_data, valid_labels = train_valid_split(train_valid_data, train_valid_labels)\n",
    "\n",
    "    # prepare the unseen test set\n",
    "    test_masks = np.array([x for x in masks if any(x.startswith(prefix) for prefix in unseen_test_set)])\n",
    "    test_masks = [os.path.join(masksp, m) for m in test_masks]\n",
    "    test_data, test_labels = prep_data(test_masks)\n",
    "\n",
    "\n",
    "    print('\\nStarting trial {}'.format(trial + 1))\n",
    "    print('Unseen Test set is:\\n')\n",
    "    print(unseen_test_set)\n",
    "\n",
    "    trial_output = os.path.join(tempout, 'trial' + str(trial + 1))\n",
    "    if not os.path.exists(trial_output):\n",
    "        os.mkdir(trial_output)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=os.path.join(trial_output, 'logs'))\n",
    "    for sets in range(runs):  # number of runs for each trial\n",
    "        output = os.path.join(trial_output, 'best' + str(sets))\n",
    "        if not os.path.exists(output):\n",
    "            os.mkdir(output)\n",
    "\n",
    "        best_auc_v = 0\n",
    "        best_auc = 0\n",
    "        best_loss = 100000.\n",
    "        best_f1_v = 0.\n",
    "        best_Acc = 0.\n",
    "\n",
    "        trans = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(degrees=90),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        trans_Valid = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        train_dset1 = TileDataset(train_data, train_labels, trans,version=0)\n",
    "        train_dset2 = TileDataset(train_data, train_labels, trans, version=1)\n",
    "        train_dset = ConcatDataset([train_dset1, train_dset2])\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dset,\n",
    "            batch_size=batch_size, shuffle=True,\n",
    "            num_workers=workers, pin_memory=False)\n",
    "\n",
    "        # validation set\n",
    "        val_dset = TileDataset(valid_data, valid_labels, trans_Valid, version=0)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dset,\n",
    "            batch_size=batch_size, shuffle=False,\n",
    "            num_workers=workers, pin_memory=False)\n",
    "\n",
    "        # test set\n",
    "        test_dset = TileDataset(test_data, test_labels, trans_Valid, version=0)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dset,\n",
    "            batch_size=batch_size, shuffle=False,\n",
    "            num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "        # cnn resNet\n",
    "        model = models.resnet50(pretrained=True)  # pretrained resnet\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, numClasses)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = FocalLoss(gamma=focal_loss_gamma).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        epoch = 0\n",
    "\n",
    "        for epoch in range(nepochs):\n",
    "            print('before training')\n",
    "            loss, acc, = train(epoch, train_loader, model, criterion, optimizer)\n",
    "            print('Training\\tEpoch : [{}]\\tLoss: {:0.4f}\\tAccuracy: {:3d}'.\n",
    "                  format(epoch, loss, int(acc * 100)))\n",
    "\n",
    "            # Log training loss\n",
    "            writer.add_scalar('Loss/Training', loss, epoch)\n",
    "\n",
    "            if (epoch + 1) % test_every == 0:\n",
    "                val_probs, val_loss, val_acc, val_preds = inference(epoch, val_loader, model, criterion)\n",
    "                print('Valdiation Set: Epoch {:3d}, Loss {:.4f}, Acccuracy {:2d}'.format(epoch + 1, val_loss,\n",
    "                                                                                         int(val_acc * 100)))\n",
    "\n",
    "                ###########\n",
    "                f1_valid, roc_auc_valid, precision_valid, recall_valid = calc_metrics(val_dset.targets, val_preds, val_probs)\n",
    "\n",
    "                print('Validation: AP-score(f1 micro): {:0.2f}\\t'.format(f1_valid))\n",
    "                print('Validation: ROC-AUC: {:0.2f}\\t'.format(roc_auc_valid))\n",
    "\n",
    "                # Log validation F1 score and accuracy\n",
    "                writer.add_scalar('Metrics/Validation_F1_Score', f1_valid, epoch)\n",
    "                writer.add_scalar('Metrics/Validation_Accuracy', val_acc, epoch)\n",
    "\n",
    "\n",
    "                if roc_auc_valid > best_auc_v:\n",
    "                    best_f1_v = f1_valid\n",
    "                    best_auc_v = roc_auc_valid\n",
    "                    best_loss = val_loss\n",
    "                    best_acc = val_acc\n",
    "\n",
    "                    obj = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'best_ap_v': best_f1_v,\n",
    "                        'best_auc_v': best_auc_v,\n",
    "                        'optimizer': optimizer.state_dict()\n",
    "                    }\n",
    "                    torch.save(obj, os.path.join(output, 'checkpoint_best_AUC.pth'))\n",
    "\n",
    "        writer.close()\n",
    "        # test set inference after completing the train epochs for one fold\n",
    "        ch = torch.load(os.path.join(output, 'checkpoint_best_AUC.pth'))\n",
    "        model.load_state_dict(ch['state_dict'])\n",
    "\n",
    "        # infernece\n",
    "        test_probs, test_loss, test_acc, test_preds = inference(epoch, test_loader, model, criterion)\n",
    "        print('Test Set: Epoch {:3d}, Loss {:.4f}, Acccuracy {:2d}'.format(epoch + 1, test_loss,\n",
    "                                                                           int(test_acc * 100)))\n",
    "\n",
    "        f1_test, roc_auc_test, precision_test,  recall_test = calc_metrics(test_dset.targets, test_preds, test_probs)\n",
    "        print('Test: AP-score(f1 micro): {:0.2f}\\t Precision: {:0.2f}\\t recall: {:0.2f}\\t'.format(f1_test, precision_test, recall_test))\n",
    "        print('Test: ROC-AUC: {:0.2f}\\t'.format(roc_auc_test))\n",
    "\n",
    "        aucPr_macro, aucPR_micro = calc_AUCPR(test_dset.targets, test_probs)\n",
    "        acc_per_class, f1_per_class = calculate_accuracy_f1_perClass(test_preds, test_dset.targets)\n",
    "\n",
    "\n",
    "        append_to_csv(trial, epoch, unseen_test_set, round(roc_auc_test, 2), round(f1_test, 2), round(test_acc, 2),\n",
    "                      round(precision_test, 2), round(recall_test, 2), aucPr_macro, aucPR_micro, acc_per_class, f1_per_class)\n",
    "\n",
    "        plt.close()\n",
    "        conf_matrix = confusion_matrix(test_dset.targets, test_preds)\n",
    "        class_names = list(labelDict.keys())\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Test results for fold ' + str(trial))\n",
    "        plt.savefig(\n",
    "            trial_output + '/Confusion_matrix_trial' + str(trial) + 'E' + str(epoch) + 'run' + str(sets) + '.png')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
